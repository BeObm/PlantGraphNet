{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:02:34.250469Z",
     "start_time": "2025-01-17T12:02:33.193843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "from skimage import segmentation"
   ],
   "id": "f0916a77a7bb76c8",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:03:28.070281Z",
     "start_time": "2025-01-17T12:03:28.050791Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ComputeSegments(nn.Module):\n",
    "    def __init__(self, segment_algorithm='quickshift',**kwargs):\n",
    "        super(ComputeSegments, self).__init__()\n",
    "        self.segment_algorithm = segment_algorithm\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def forward(self, sample):\n",
    "        image = sample['image']\n",
    "\n",
    "        if self.segment_algorithm == 'quickshift':\n",
    "            segments = segmentation.quickshift(image,**self.kwargs)\n",
    "        elif self.segment_algorithm == 'slic':\n",
    "            segments = segmentation.slic(image,**self.kwargs)\n",
    "        elif self.segment_algorithm == 'felzenszwalb':\n",
    "            segments = segmentation.felzenszwalb(image,**self.kwargs)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported segment algorithm. Choose from 'quickshift', 'slic', or 'felzenszwalb'.\")\n",
    "\n",
    "        sample['segments'] = segments\n",
    "        return sample"
   ],
   "id": "cd5c1ddb7055c780",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:18:20.090568Z",
     "start_time": "2025-01-17T13:18:20.077468Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f828425741a832e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:19:26.797063Z",
     "start_time": "2025-01-17T13:19:22.962321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def image_to_graph(image):\n",
    "    \"\"\"\n",
    "    Convert an image into a graph representation.\n",
    "\n",
    "    Parameters:\n",
    "        image (torch.Tensor): An input image of shape (H, W, C), where H is the height,\n",
    "                              W is the width, and C is the number of channels (e.g., 3 for RGB).\n",
    "\n",
    "    Returns:\n",
    "        adjacency_matrix (torch.Tensor): Adjacency matrix of size (N, N) where N = H*W.\n",
    "        node_features (torch.Tensor): Node features of the graph of size (N, C).\n",
    "    \"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Convert numpy array to torch tensor if needed\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "    if image.dim() != 3:\n",
    "        raise ValueError(\"Input image must have 3 dimensions: (H, W, C)\")\n",
    "\n",
    "    H, W, C = image.shape\n",
    "    N = H * W  # Total number of nodes/pixels\n",
    "\n",
    "    # Flatten the image into a list of nodes\n",
    "    node_features = image.view(-1, C)  # Shape: (N, C)\n",
    "\n",
    "    # Initialize adjacency matrix\n",
    "    adjacency_matrix = torch.zeros((N, N), dtype=torch.float32)\n",
    "\n",
    "    # Compute edges for all 8-connectivity neighbors\n",
    "    pixel_to_index = lambda x, y: x * W + y\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            current_index = pixel_to_index(i, j)\n",
    "\n",
    "            # Check all 8 neighbors\n",
    "            neighbors = [\n",
    "                (i - 1, j),    # Top\n",
    "                (i + 1, j),    # Bottom\n",
    "                (i, j - 1),    # Left\n",
    "                (i, j + 1),    # Right\n",
    "                (i - 1, j - 1),  # Top-left\n",
    "                (i - 1, j + 1),  # Top-right\n",
    "                (i + 1, j - 1),  # Bottom-left\n",
    "                (i + 1, j + 1),  # Bottom-right\n",
    "            ]\n",
    "\n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < H and 0 <= nj < W:  # Ensure within bounds\n",
    "                    neighbor_index = pixel_to_index(ni, nj)\n",
    "                    # Connect current pixel to its neighbor\n",
    "                    adjacency_matrix[current_index, neighbor_index] = 1\n",
    "\n",
    "    return adjacency_matrix, node_features\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example image (5x5x3 RGB image with random values)\n",
    "    example_image = torch.randint(0, 256, (5, 5, 3), dtype=torch.uint8)\n",
    "\n",
    "    # Convert to graph\n",
    "    adjacency_matrix, node_features = image_to_graph(example_image)\n",
    "\n",
    "    print(\"Adjacency Matrix:\")\n",
    "    print(adjacency_matrix)\n",
    "    print(\"\\nNode Features:\")\n",
    "    print(node_features)"
   ],
   "id": "7b42498722c04391",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency Matrix:\n",
      "tensor([[0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
      "         1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "         1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "         0., 0., 1., 1., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0.,\n",
      "         1., 0., 0., 1., 1., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "         0., 1., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "         0., 0., 1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "         1., 0., 0., 1., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         1., 1., 0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 1., 0., 0., 0., 1., 0.]])\n",
      "\n",
      "Node Features:\n",
      "tensor([[ 33, 226, 133],\n",
      "        [218, 231, 195],\n",
      "        [140, 159,   7],\n",
      "        [ 92, 193, 137],\n",
      "        [ 75,   2, 178],\n",
      "        [175,  97, 212],\n",
      "        [145, 233, 156],\n",
      "        [179,  58, 237],\n",
      "        [240, 121,  13],\n",
      "        [ 81, 137,  27],\n",
      "        [215, 113,  17],\n",
      "        [229, 255, 123],\n",
      "        [  4,  87, 210],\n",
      "        [ 21, 112,  23],\n",
      "        [ 70, 133, 104],\n",
      "        [241, 250, 111],\n",
      "        [223, 135, 117],\n",
      "        [100, 254,   5],\n",
      "        [  1,  53,  72],\n",
      "        [194, 161, 135],\n",
      "        [ 57, 203, 111],\n",
      "        [217, 190,   9],\n",
      "        [ 29,   2, 252],\n",
      "        [ 98, 115,  36],\n",
      "        [ 69, 158, 128]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:30:21.502796Z",
     "start_time": "2025-01-17T13:30:21.484302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from skimage import io\n",
    "\n",
    "image= io.imread(r\"C:\\Users\\au783153\\Documents\\OBM\\CODES\\HeathlandSpeciesClassifier\\dataset\\images\\train\\calluna\\im107_104_4.1.0.jpg\")"
   ],
   "id": "7536dd9517cf0b3a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:44:32.483246Z",
     "start_time": "2025-01-17T13:44:32.205972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "def validate_image(image):\n",
    "    \"\"\"Validate the input image.\"\"\"\n",
    "    if isinstance(image, np.ndarray):\n",
    "        # Convert numpy array to torch tensor if needed\n",
    "        image = torch.from_numpy(image)\n",
    "    if image.dim() != 3:\n",
    "        raise ValueError(\"Input image must have 3 dimensions: (height, width, channels)\")\n",
    "    return image\n",
    "\n",
    "\n",
    "def pixel_to_index(x, y, width):\n",
    "    \"\"\"Convert 2D pixel coordinates to a flattened index.\"\"\"\n",
    "    return x * width + y\n",
    "\n",
    "\n",
    "def compute_edges(height, width):\n",
    "    \"\"\"Generate an edge list for a graph based on 8-connectivity.\"\"\"\n",
    "    edges = []\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            current_index = pixel_to_index(i, j, width)\n",
    "            neighbors = [\n",
    "                (i - 1, j),  # Top\n",
    "                (i + 1, j),  # Bottom\n",
    "                (i, j - 1),  # Left\n",
    "                (i, j + 1),  # Right\n",
    "                (i - 1, j - 1),  # Top-left\n",
    "                (i - 1, j + 1),  # Top-right\n",
    "                (i + 1, j - 1),  # Bottom-left\n",
    "                (i + 1, j + 1),  # Bottom-right\n",
    "            ]\n",
    "            # Collect valid edges\n",
    "            for ni, nj in neighbors:\n",
    "                if 0 <= ni < height and 0 <= nj < width:  # Ensure within bounds\n",
    "                    neighbor_index = pixel_to_index(ni, nj, width)\n",
    "                    edges.append((current_index, neighbor_index))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def image_to_graph(image):\n",
    "    \"\"\"\n",
    "    Convert an image into a graph representation using an edge list.\n",
    "    Parameters:\n",
    "        image (torch.Tensor): An input image of shape (height, width, channels),\n",
    "                              where 'channels' is typically 3 for RGB images.\n",
    "    Returns:\n",
    "        edges (list of tuple): List of edges, where each edge is a tuple (node1, node2).\n",
    "        node_features (torch.Tensor): Node features of the graph of size (num_pixels, channels).\n",
    "    \"\"\"\n",
    "    image = validate_image(image)\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    # Flatten the image into a list of nodes\n",
    "    node_features = image.view(-1, channels)  # Shape: (num_pixels, channels)\n",
    "\n",
    "    # Create an edge list for the graph\n",
    "    edges = compute_edges(height, width)\n",
    "\n",
    "    return edges, node_features\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example image (5x5x3 RGB image with random values)\n",
    "    example_image = image\n",
    "\n",
    "    # Convert to graph\n",
    "    edges, node_features = image_to_graph(example_image)\n",
    "\n",
    "    print(\"Edge List:\")\n",
    "    print(len(edges))\n",
    "    print(\"\\nNode Features:\")\n",
    "    print(node_features)\n"
   ],
   "id": "bf96880504905191",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge List:\n",
      "178204\n",
      "\n",
      "Node Features:\n",
      "tensor([[ 73,  71,  48],\n",
      "        [ 72,  70,  47],\n",
      "        [ 75,  73,  50],\n",
      "        ...,\n",
      "        [113, 105,  82],\n",
      "        [103,  92,  70],\n",
      "        [120, 108,  86]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:36:10.282843Z",
     "start_time": "2025-01-17T13:36:09.932453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "\n",
    "# Define constant for neighbor offsets\n",
    "NEIGHBOR_OFFSETS = [\n",
    "    (-1, 0),  # Top\n",
    "    (1, 0),  # Bottom\n",
    "    (0, -1),  # Left\n",
    "    (0, 1),  # Right\n",
    "    (-1, -1),  # Top-left\n",
    "    (-1, 1),  # Top-right\n",
    "    (1, -1),  # Bottom-left\n",
    "    (1, 1),  # Bottom-right\n",
    "]\n",
    "\n",
    "\n",
    "def validate_image(input_image):\n",
    "    \"\"\"Validate the input image.\"\"\"\n",
    "    if isinstance(input_image, np.ndarray):\n",
    "        # Convert numpy array to torch tensor if needed\n",
    "        input_image = torch.from_numpy(input_image)\n",
    "    if input_image.dim() != 3:\n",
    "        raise ValueError(\"Input image must have 3 dimensions: (height, width, channels)\")\n",
    "    return input_image\n",
    "\n",
    "\n",
    "def pixel_to_index(x, y, width):\n",
    "    \"\"\"Convert 2D pixel coordinates to a flattened index.\"\"\"\n",
    "    return x * width + y\n",
    "\n",
    "\n",
    "def generate_neighbors(row, col, height, width):\n",
    "    \"\"\"Generate valid neighboring indices for a pixel.\"\"\"\n",
    "    neighbors = []\n",
    "    for offset_row, offset_col in NEIGHBOR_OFFSETS:\n",
    "        neighbor_row, neighbor_col = row + offset_row, col + offset_col\n",
    "        if 0 <= neighbor_row < height and 0 <= neighbor_col < width:\n",
    "            neighbors.append((neighbor_row, neighbor_col))\n",
    "    return neighbors\n",
    "\n",
    "\n",
    "def compute_edges(height, width):\n",
    "    \"\"\"Generate an edge list for a graph based on 8-connectivity.\"\"\"\n",
    "    edges = []\n",
    "    for row in range(height):\n",
    "        for col in range(width):\n",
    "            current_index = pixel_to_index(row, col, width)\n",
    "            for neighbor_row, neighbor_col in generate_neighbors(row, col, height, width):\n",
    "                neighbor_index = pixel_to_index(neighbor_row, neighbor_col, width)\n",
    "                edges.append((current_index, neighbor_index))\n",
    "    return edges\n",
    "\n",
    "\n",
    "def image_to_graph(image):\n",
    "    \"\"\"\n",
    "    Convert an image into a graph representation using an edge list.\n",
    "    Parameters:\n",
    "        image (torch.Tensor): An input image of shape (height, width, channels),\n",
    "                              where 'channels' is typically 3 for RGB images.\n",
    "    Returns:\n",
    "        edges (list of tuple): List of edges, where each edge is a tuple (node1, node2).\n",
    "        flattened_pixels (torch.Tensor): Node features of the graph of size (num_pixels, channels).\n",
    "    \"\"\"\n",
    "    image = validate_image(image)\n",
    "    height, width, channels = image.shape\n",
    "    # Flatten the image into a list of nodes\n",
    "    flattened_pixels = image.view(-1, channels)  # Shape: (num_pixels, channels)\n",
    "    # Create an edge list for the graph\n",
    "    edges = compute_edges(height, width)\n",
    "    return edges, flattened_pixels\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Placeholder for example image\n",
    "    example_image = image\n",
    "    try:\n",
    "        # Convert to graph\n",
    "        edges, flattened_pixels = image_to_graph(example_image)\n",
    "        print(\"Edge List:\")\n",
    "        print(len(edges))\n",
    "        print(\"\\nNode Features:\")\n",
    "        print(flattened_pixels)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n"
   ],
   "id": "a0181418a4c71485",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edge List:\n",
      "178204\n",
      "\n",
      "Node Features:\n",
      "tensor([[ 73,  71,  48],\n",
      "        [ 72,  70,  47],\n",
      "        [ 75,  73,  50],\n",
      "        ...,\n",
      "        [113, 105,  82],\n",
      "        [103,  92,  70],\n",
      "        [120, 108,  86]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "3881552ac7938e4c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T13:27:45.637900Z",
     "start_time": "2025-01-17T13:27:45.611590Z"
    }
   },
   "cell_type": "code",
   "source": "e.shape",
   "id": "8136d12c91bac36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2e8fae36175dfd61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
